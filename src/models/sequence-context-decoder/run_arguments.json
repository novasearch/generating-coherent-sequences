{
    "experiment_name": "debug",
    "reload_optimizer": false,
    "save_optimizer": false,
    "cache_dir": null,
    "run_name": "C1-Current-Step-Cosine-05",
    "optim": "adamw_torch",
    "model_max_length": 400,
    "output_dir": "./models/C1-Current-Step-Cosine-05",
    "project_name": "alpaca-sequence-illustration",
    "num_train_epochs": 10.0,
    "per_device_train_batch_size": 2,
    "per_device_eval_batch_size": 2,
    "save_strategy": "epoch",
    "evaluation_strategy": "no",
    "run_type": "train",
    "report_to_wandb": true,
    "seed": 11731,
    "stop_criteria": -1.0,
    "lr_scheduler_type": "cosine",
    "lr_step_size": 250,
    "perpetual": false,
    "debug": false,
    "max_steps": -1,
    "infer_checkpoints": false,
    "learning_rate": 1e-05,
    "mixed_precision": "BF16",
    "gradient_accumulation_steps": 4,
    "warmup_steps": 0,
    "logging_steps": 1,
    "eval_steps": 500,
    "save_steps": 500,
    "save_total_limit": 10,
    "overwrite_output_dir": true,
    "weight_decay": 0.01,
    "adam_beta1": 0.9,
    "adam_beta2": 0.999,
    "adam_epsilon": 1e-08,
    "warmup_ratio": 0.03,
    "warmup_before_inference": 0,
    "param_update_strategy": "epoch",
    "param_update_interval": 500,
    "parallel_type": "NO",
    "use_dpo": false,
    "dpo_mixed": false,
    "dpo_beta": 0.1,
    "dpo_theta": 0.2,
    "interval_dpo": false,
    "interval_dpo_steps": 4,
    "lora": true
}